{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b690218-f44f-4ead-988b-0337c180e2e4",
   "metadata": {},
   "source": [
    "# Leaderboards for all 32 Benchmarks in the IR Experiment Platform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc261d4-182a-4f5f-a6cd-0de40f87c940",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f2b741-c83c-4600-825e-55aaaf9f0253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tira.rest_api_client import Client\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from trectools import TrecQrel, TrecRun, TrecEval\n",
    "from statistics import mean\n",
    "import os\n",
    "\n",
    "tira = Client()\n",
    "\n",
    "# We organize the pilot benchmarks in the IR Experiment platform inside one \"virtual pseudo shared task\"\n",
    "TASK = 'ir-benchmarks'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad7836-8e18-4947-9c8c-959ce0315686",
   "metadata": {},
   "source": [
    "### Create the Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "315d5cea-c006-411f-ad1f-d9d6055c80b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}\n",
      "\\centering\n",
      "\\small\n",
      "\\renewcommand{\\arraystretch}{0.8}%\n",
      "\\setlength{\\tabcolsep}{3.2pt}%\n",
      "\\caption{The effectiveness of the 50~retrieval models on the 31~benchmarks (Touch{\\'e}~23 is excluded as this shared task is still ongoing). We report the nDCG@10 scores for selected models (BM25, ColBERT, TAS-B, monoT5, and three duoT5 variants) and the best, median, and worst submission for the groups of the 20~lexical models and the 17~bi-encoder models. For corpora that have multiple benchmarks, we report the macro average.}\n",
      "\\label{table-retrieval-effectiveness}\n",
      "\\vspace{-2ex}\n",
      "\\begin{tabular}{@{}lrlcllllllllllllll@{}}\n",
      "\\toprule\n",
      "\\bfseries Corpus & \\bfseries ChatNoir & \\multicolumn{4}{@{}c@{}}{\\bfseries Lexical} & \\bfseries Late Int. & \\multicolumn{4}{@{}c@{}}{\\bfseries Bi-Encoder} & \\multicolumn{3}{@{}c@{}}{\\bfseries duoT5} & \\multicolumn{4}{@{}c@{}}{\\bfseries PyGaggle}\\\\\n",
      "\n",
      "\\cmidrule(r@{\\tabcolsep}){2-2}\n",
      "\\cmidrule(r@{\\tabcolsep}){3-6}\n",
      "\\cmidrule(r@{\\tabcolsep}){7-7}\n",
      "\\cmidrule(r@{\\tabcolsep}){8-11}\n",
      "\\cmidrule(r@{\\tabcolsep}){12-14}\n",
      "\\cmidrule{15-18}\n",
      "&  & BM25 & Best & Median & Worst & ColBERT & TAS-B & Best & Median & Worst & Base & Large & 3b & MonoT5 & Best & Median & Worst \\\\\n",
      "\n",
      "\\midrule\n",
      "\n",
      "Antique &  --- & 0.51 & 0.53 & 0.51 & 0.36 & 0.47 & 0.40 & 0.49 & 0.44 & 0.30 & 0.54 & 0.46 & 0.52 & 0.51 & \\textbf{0.54} & 0.51 & 0.45 \\\\\n",
      "\n",
      "\n",
      "Args.me &  --- & 0.43 & \\textbf{0.57} & 0.43 & 0.14 & 0.26 & 0.17 & 0.33 & 0.24 & 0.13 & 0.33 & 0.29 & 0.29 & 0.30 & 0.39 & 0.34 & 0.27 \\\\\n",
      "\n",
      "\n",
      "CORD-19 &  --- & 0.28 & 0.64 & 0.55 & 0.21 & 0.58 & 0.50 & \\textbf{0.70} & 0.60 & 0.50 & 0.66 & 0.61 & 0.66 & 0.69 & 0.69 & 0.63 & 0.55 \\\\\n",
      "\n",
      "\n",
      "ClueWeb09 & 0.16 & 0.18 & \\textbf{0.24} & 0.18 & 0.12 & 0.17 & 0.16 & 0.20 & 0.17 & 0.13 & 0.15 & 0.15 & 0.18 & 0.17 & 0.19 & 0.17 & 0.12 \\\\\n",
      "\n",
      "\n",
      "ClueWeb12 & \\textbf{0.36} & 0.24 & 0.27 & 0.25 & 0.14 & 0.23 & 0.25 & 0.28 & 0.26 & 0.23 & 0.33 & 0.30 & 0.35 & 0.26 & 0.28 & 0.26 & 0.23 \\\\\n",
      "\n",
      "\n",
      "Cranfield &  --- & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\\\\n",
      "\n",
      "\n",
      "Disks4+5 &  --- & 0.44 & 0.46 & 0.44 & 0.37 & 0.46 & 0.39 & 0.49 & 0.43 & 0.37 & 0.45 & 0.38 & 0.44 & 0.53 & \\textbf{0.57} & 0.53 & 0.43 \\\\\n",
      "\n",
      "\n",
      "Gov &  --- & 0.22 & 0.24 & 0.22 & 0.15 & 0.23 & 0.22 & 0.27 & 0.24 & 0.21 & 0.19 & 0.15 & 0.22 & 0.26 & \\textbf{0.29} & 0.26 & 0.22 \\\\\n",
      "\n",
      "\n",
      "Gov2 &  --- & 0.47 & 0.49 & 0.44 & 0.25 & 0.45 & 0.34 & 0.46 & 0.42 & 0.34 & 0.47 & 0.43 & 0.48 & 0.48 & \\textbf{0.51} & 0.48 & 0.41 \\\\\n",
      "\n",
      "\n",
      "MARCO &  --- & 0.49 & 0.50 & 0.48 & 0.37 & 0.69 & 0.64 & 0.71 & 0.66 & 0.64 & 0.64 & 0.57 & 0.63 & 0.71 & \\textbf{0.74} & 0.71 & 0.63 \\\\\n",
      "\n",
      "\n",
      "Medline &  --- & 0.34 & \\textbf{0.42} & 0.27 & 0.18 & 0.25 & 0.14 & 0.26 & 0.21 & 0.14 & 0.34 & 0.32 & 0.36 & 0.25 & 0.35 & 0.27 & 0.24 \\\\\n",
      "\n",
      "\n",
      "NFCorpus &  --- & 0.27 & 0.28 & 0.27 & 0.26 & 0.27 & 0.25 & 0.29 & 0.26 & 0.24 & 0.28 & 0.24 & 0.29 & 0.30 & \\textbf{0.31} & 0.30 & 0.28 \\\\\n",
      "\n",
      "\n",
      "Vaswani &  --- & 0.45 & 0.46 & 0.45 & 0.30 & 0.43 & 0.34 & 0.44 & 0.38 & 0.22 & 0.41 & 0.34 & 0.46 & 0.31 & \\textbf{0.48} & 0.41 & 0.08 \\\\\n",
      "\n",
      "\n",
      "WaPo &  --- & 0.38 & 0.39 & 0.37 & 0.24 & 0.43 & 0.34 & 0.43 & 0.37 & 0.33 & 0.40 & 0.28 & 0.40 & 0.45 & \\textbf{0.49} & 0.45 & 0.40 \\\\\n",
      "\n",
      "\n",
      "\\midrule\n",
      "\n",
      "Avg. &  & 0.34 & 0.39 & 0.35 & 0.22 & 0.35 & 0.30 & 0.38 & 0.33 & 0.27 & 0.37 & 0.32 & 0.38 & 0.37 & 0.42 & 0.38 & 0.31\\\\\n",
      "\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table*}\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from statistics import mean, median\n",
    "\n",
    "corpus_to_benchmarks = json.load(open('corpus-to-benchmark.json'))\n",
    "all_evaluations = json.load(open('all_evaluations.json'))\n",
    "types_of_retrieval_softwares = json.load(open('type-of-retrieval-softwares.json'))\n",
    "\n",
    "scores_per_approach = {}\n",
    "\n",
    "best_scores = {\n",
    "    'Antique': 0.54,\n",
    "    'Args.me': 0.56,\n",
    "    'CORD-19': 0.695,\n",
    "    'ClueWeb09': 0.23,\n",
    "    'ClueWeb12': 0.36,\n",
    "    'Cranfield': 0.011,\n",
    "    'Disks4+5': 0.56,\n",
    "    'Gov': 0.28,\n",
    "    'Gov2': 0.50,\n",
    "    'MARCO': 0.73,\n",
    "    'Medline': 0.41,\n",
    "    'NFCorpus': 0.31,\n",
    "    'Vaswani': 0.47,\n",
    "    'WaPo': 0.49\n",
    "}\n",
    "\n",
    "def aggregate_score_for_corpus(measure, corpus, approach):\n",
    "    benchmarks = corpus_to_benchmarks[corpus]\n",
    "    ret = []\n",
    "    \n",
    "    for benchmark in set(benchmarks):\n",
    "        if approach not in all_evaluations[benchmark] or not all_evaluations[benchmark][approach] or not all_evaluations[benchmark][approach][measure]:\n",
    "            continue\n",
    "        ret += [all_evaluations[benchmark][approach][measure]]\n",
    "    \n",
    "    return (None, 0, len(benchmarks)) if len(ret) == 0 else (mean(ret), len(ret), len(benchmarks))\n",
    "\n",
    "def format_s(i, corpus, approach):\n",
    "    scores, included, expected = i\n",
    "    if type(scores) == list:\n",
    "        return f(mean(scores), sum(included), sum(expected))\n",
    "    \n",
    "    if scores is None:\n",
    "        return '---'\n",
    "\n",
    "    if approach not in scores_per_approach:\n",
    "        scores_per_approach[approach] = {}\n",
    "    \n",
    "    assert corpus not in scores_per_approach[approach]\n",
    "    \n",
    "    scores_per_approach[approach][corpus] = scores\n",
    "    \n",
    "    ret = '{0:.2f}'.format(scores)\n",
    "    \n",
    "    if included != expected:\n",
    "        return '{\\\\color{red} ' + ret + ' }'\n",
    "    elif scores >= best_scores[corpus]:\n",
    "        return '\\\\textbf{' + ret + '}'\n",
    "    else:\n",
    "        return ret\n",
    "\n",
    "def report_best_median_worst(corpus, measure, group):\n",
    "    scores = []\n",
    "    expected = []\n",
    "    included = []\n",
    "    \n",
    "    nones = 0\n",
    "    \n",
    "    for software in types_of_retrieval_softwares[group]:\n",
    "        tmp = aggregate_score_for_corpus(measure, corpus, software)\n",
    "        if tmp is None:\n",
    "            nones += 1\n",
    "            continue\n",
    "        if tmp[0] is None:\n",
    "            nones += 1\n",
    "            continue\n",
    "\n",
    "        scores += [tmp[0]]\n",
    "        included += [tmp[1]]\n",
    "        expected += [tmp[2]]\n",
    "    \n",
    "    if sum(expected) != sum(included) or nones > 0:\n",
    "        expected = 2\n",
    "        included = 1\n",
    "    \n",
    "    return format_s([max(scores), expected, included], corpus, group + '-best') + \\\n",
    "        ' & ' + format_s([median(scores), expected, included], corpus, group + '-median') + \\\n",
    "        ' & ' + format_s([min(scores), expected, included], corpus, group + '-worst')\n",
    "\n",
    "def table_line(corpus, measure):\n",
    "    expected_evaluations = len(corpus_to_benchmarks[corpus])\n",
    "    ret = corpus + ' & '\n",
    "    \n",
    "    if 'clueweb' in corpus.lower():\n",
    "        ret += format_s(aggregate_score_for_corpus(measure, corpus, 'ChatNoir'), corpus, 'ChatNoir') + ' & '\n",
    "    else:\n",
    "        ret += ' --- & ' \n",
    "    \n",
    "    ret += format_s(aggregate_score_for_corpus(measure, corpus, 'BM25 Re-Rank (tira-ir-starter-pyterrier)'), corpus, 'BM25') + ' & '\n",
    "    \n",
    "    ret += report_best_median_worst(corpus, measure, 'Lexical')\n",
    "    \n",
    "    ret += ' & ' + format_s(aggregate_score_for_corpus(measure, corpus, 'ColBERT Re-Rank (tira-ir-starter-pyterrier)'), corpus, 'ColBERT') + ' & '\n",
    "    \n",
    "    ret += format_s(aggregate_score_for_corpus(measure, corpus, 'TASB msmarco-distilbert-base-cos (tira-ir-starter-beir)'), corpus, 'TASB') + ' & '\n",
    "    \n",
    "    ret += report_best_median_worst(corpus, measure, 'Bi-Encoder')\n",
    "    \n",
    "    ret += ' & ' + format_s(aggregate_score_for_corpus(measure, corpus, \"DuoT5 base-10k-ms-marco Top-25 (tira-ir-starter-pyterrier)\"), corpus, 'DT5-base') + ' & '\n",
    "    ret += format_s(aggregate_score_for_corpus(measure, corpus, \"DuoT5 Top-25 (tira-ir-starter-pyterrier)\"), corpus, 'DT5') + ' & '\n",
    "    ret += format_s(aggregate_score_for_corpus(measure, corpus, \"DuoT5 3b-ms-marco Top-25 (tira-ir-starter-pyterrier)\"), corpus, 'DT5-3b') + ' & '\n",
    "    \n",
    "    \n",
    "    ret += format_s(aggregate_score_for_corpus(measure, corpus, 'MonoT5 Base (tira-ir-starter-gygaggle)'), corpus, 'MonoT5') + ' & '\n",
    "    \n",
    "    ret += report_best_median_worst(corpus, measure, 'PyGaggle')\n",
    "    \n",
    "    return ret + ' \\\\\\\\\\n'\n",
    "\n",
    "def format_avg(approach):\n",
    "    return '{0:.2f}'.format(mean(scores_per_approach[approach].values()))\n",
    "\n",
    "def create_table(measure):\n",
    "    return \"\"\"\\\\begin{table*}\n",
    "\\\\centering\n",
    "\\\\small\n",
    "\\\\renewcommand{\\\\arraystretch}{0.8}%\n",
    "\\\\setlength{\\\\tabcolsep}{3.2pt}%\n",
    "\\\\caption{The effectiveness of the 50~retrieval models on the 31~benchmarks (Touch{\\\\'e}~23 is excluded as this shared task is still ongoing). We report the nDCG@10 scores for selected models (BM25, ColBERT, TAS-B, monoT5, and three duoT5 variants) and the best, median, and worst submission for the groups of the 20~lexical models and the 17~bi-encoder models. For corpora that have multiple benchmarks, we report the macro average.}\n",
    "\\\\label{table-retrieval-effectiveness}\n",
    "\\\\vspace{-2ex}\n",
    "\\\\begin{tabular}{@{}lrlcllllllllllllll@{}}\n",
    "\\\\toprule\n",
    "\\\\bfseries Corpus & \\\\bfseries ChatNoir & \\\\multicolumn{4}{@{}c@{}}{\\\\bfseries Lexical} & \\\\bfseries Late Int. & \\\\multicolumn{4}{@{}c@{}}{\\\\bfseries Bi-Encoder} & \\\\multicolumn{3}{@{}c@{}}{\\\\bfseries duoT5} & \\\\multicolumn{4}{@{}c@{}}{\\\\bfseries PyGaggle}\\\\\\\\\n",
    "\n",
    "\\\\cmidrule(r@{\\\\tabcolsep}){2-2}\n",
    "\\\\cmidrule(r@{\\\\tabcolsep}){3-6}\n",
    "\\\\cmidrule(r@{\\\\tabcolsep}){7-7}\n",
    "\\\\cmidrule(r@{\\\\tabcolsep}){8-11}\n",
    "\\\\cmidrule(r@{\\\\tabcolsep}){12-14}\n",
    "\\\\cmidrule{15-18}\n",
    "&  & BM25 & Best & Median & Worst & ColBERT & TAS-B & Best & Median & Worst & Base & Large & 3b & MonoT5 & Best & Median & Worst \\\\\\\\\n",
    "\n",
    "\\\\midrule\n",
    "\n",
    "\"\"\" + \"\\n\\n\".join(table_line(i, measure) for i in sorted(list(corpus_to_benchmarks.keys()))) + \"\"\"\n",
    "\n",
    "\\\\midrule\n",
    "\n",
    "Avg. &  & \"\"\" + (' & '.join(format_avg(i) for i in ['BM25', 'Lexical-best', 'Lexical-median', 'Lexical-worst', 'ColBERT', 'TASB', 'Bi-Encoder-best', 'Bi-Encoder-median', 'Bi-Encoder-worst', 'DT5-base', 'DT5', 'DT5-3b', 'MonoT5', 'PyGaggle-best', 'PyGaggle-median', 'PyGaggle-worst'])) + \"\"\"\\\\\\\\\n",
    "\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table*}\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "print(create_table('ndcg@10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf5662a-6e3c-4b80-b079-109322287bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BM25', 'Lexical-best', 'Lexical-median', 'Lexical-worst', 'ColBERT', 'TASB', 'Bi-Encoder-best', 'Bi-Encoder-median', 'Bi-Encoder-worst', 'DT5-base', 'DT5', 'DT5-3b', 'MonoT5', 'PyGaggle-best', 'PyGaggle-median', 'PyGaggle-worst', 'ChatNoir'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_per_approach.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27c4d08-8bd3-4dac-8cc2-ef6b8645e296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "339a1cf4-92e6-491d-95fb-e644cc2f829a",
   "metadata": {},
   "source": [
    "### Download all submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471bfac4-b371-4497-a584-116b006b74a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load all Submissions: 100%|███████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:22<00:00,  1.40it/s]\n",
      "Load all Qrels: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1581/1581 [17:39<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "dry_run = False\n",
    "dataset_to_submissions = {}\n",
    "\n",
    "for dataset in tqdm(json.load(open('benchmarks-in-pilot-study.json')).keys(), 'Load all Submissions'):\n",
    "    dataset_to_submissions[dataset] = tira.submissions(TASK, dataset)\n",
    "\n",
    "qrels = {}\n",
    "\n",
    "for dataset in tqdm(json.load(open('benchmarks-in-pilot-study.json')).keys(), 'Load all Qrels'):\n",
    "    qrels[dataset] = TrecQrel(f'../data/qrels/{dataset}-qrels.txt')\n",
    "    \n",
    "evaluations = [(i, j) for j in json.load(open('retrieval-softwares-in-pilot-study.json')).keys() for i in dataset_to_submissions.keys()]\n",
    "evaluations_with_parsed_runs = {}\n",
    "\n",
    "for dataset, approach in tqdm(evaluations):\n",
    "    if (approach, dataset) not in evaluations_with_parsed_runs:\n",
    "        evaluations_with_parsed_runs[(approach, dataset)] = []  \n",
    "    for _, submission in dataset_to_submissions[dataset][(dataset_to_submissions[dataset]['software'] == approach) & (dataset_to_submissions[dataset]['is_evaluation'] == False)].iterrows():\n",
    "\n",
    "        src_dir = tira.download_zip_to_cache_directory(submission['task'], submission['dataset'], submission['team'], submission['run_id'])\n",
    "        \n",
    "        if dry_run:\n",
    "            if os.path.isdir(src_dir.split('/output')[0]) and not os.path.isfile(src_dir + '/run.txt'):\n",
    "                src_dir = src_dir.split('/output')[0]\n",
    "                !rm -rf {src_dir}\n",
    "        \n",
    "            if os.path.isfile(src_dir + '/run.txt'):\n",
    "                evaluations_with_parsed_runs[(approach, dataset)] += [src_dir + '/run.txt']\n",
    "        \n",
    "        if not dry_run:\n",
    "            try:\n",
    "                run = TrecRun(src_dir +'/run.txt')\n",
    "                evaluations_with_parsed_runs[(approach, dataset)] += [run]\n",
    "            except:\n",
    "                try:\n",
    "                    !rm -rf {src_dir}\n",
    "                except:\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c77be16-8fee-4c95-a42d-e590bbba77ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1581/1581 [09:12<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_run(approach, dataset):\n",
    "    if (approach, dataset) not in evaluations_with_parsed_runs or not evaluations_with_parsed_runs[(approach, dataset)]:\n",
    "        return None\n",
    "    \n",
    "    te = TrecEval(evaluations_with_parsed_runs[(approach, dataset)][0], qrels[dataset])\n",
    "    \n",
    "    return {\n",
    "        'approach': approach,\n",
    "        'dataset': dataset,\n",
    "        'ndcg@10': te.get_ndcg(depth=10),\n",
    "        'precision@10': te.get_precision(depth=10),\n",
    "        'unjudged@10': te.get_unjudged(depth=10)\n",
    "    }\n",
    "\n",
    "all_evaluations = {}\n",
    "for dataset, approach in tqdm(evaluations):\n",
    "    if dataset not in all_evaluations:\n",
    "        all_evaluations[dataset] = {}\n",
    "    \n",
    "    all_evaluations[dataset][approach] = evaluate_run(approach, dataset)\n",
    "\n",
    "json.dump(all_evaluations, open('all_evaluations.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555d5399-f95f-424b-8411-c6d07ddba662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 221259.16it/s]\n"
     ]
    }
   ],
   "source": [
    "df = {}\n",
    "\n",
    "tmp_to_execute = [k for k, v in evaluations_with_parsed_runs.items() if not v or len(v) <= 0]\n",
    "resources = json.load(open('retrieval-softwares-in-pilot-study.json'))\n",
    "to_execute = []\n",
    "\n",
    "for i in tqdm(list(tmp_to_execute)):\n",
    "    #if resources[i[0]] == 'small-resources-gpu' and (i[0] not in {'senior-platform', 'claret-fortress', 'ectilinear-credits'}):\n",
    "    if resources[i[0]] == 'small-resources-gpu':\n",
    "        prefix = 'docker-id-244-on-' if 'clueweb' not in i[1] else 'docker-id-242-on-'\n",
    "        \n",
    "        to_execute += [{'approach': f'ir-benchmarks/tira-ir-starter/{i[0]}', 'dataset': i[1], 'rerank_dataset': prefix + i[1], 'resources': 'small-resources-gpu'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f902ecd9-dbf0-409c-9957-c6a5cf6fbb88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(to_execute):\n",
    "    try:\n",
    "        if 'rectilinear-credits' in i['approach'] or 'DuoT5 base-10k-ms-marco Top-25 (tira-ir-starter-pyterrier)' in i['approach']:\n",
    "            tira.run_software(approach=i['approach'], dataset=i['dataset'], rerank_dataset=i['rerank_dataset'], resources=i['resources'])\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
